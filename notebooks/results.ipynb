{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d55c69",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b67b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import CSV from \n",
    "# # results/grayscale_sun/per_image_metrics.csv\n",
    "# # results/canny_sun/per_image_metrics.csv\n",
    "# # results/LoG_sun/per_image_metrics.csv\n",
    "# # results/random_sun/per_image_metrics.csv\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb25d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_csv(file_path):\n",
    "#     \"\"\"\n",
    "#     Load CSV file and return DataFrame.\n",
    "#     \"\"\"\n",
    "#     if os.path.exists(file_path):\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         return df\n",
    "#     else:\n",
    "#         print(f\"File {file_path} does not exist.\")\n",
    "#         return None\n",
    "    \n",
    "#     # metric,mean,std\n",
    "# def plot_metrics(df, title):\n",
    "#     \"\"\"\n",
    "#     Plot metrics from DataFrame.\n",
    "#     \"\"\"\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.barplot(x='metric', y='mean', data=df, yerr=df['std'], capsize=.2)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Metric')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3230281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the 4 csv files\n",
    "# experiment_name = 'lapa'\n",
    "# grayscale_df = load_csv('../results/grayscale_{}/per_image_metrics.csv'.format(experiment_name))\n",
    "# canny_df = load_csv('../results/canny_{}/per_image_metrics.csv'.format(experiment_name))\n",
    "# log_df = load_csv('../results/DoG_{}/per_image_metrics.csv'.format(experiment_name))\n",
    "# random_df = load_csv('../results/random_{}/per_image_metrics.csv'.format(experiment_name))\n",
    "\n",
    "# # Combine the dataframes\n",
    "# combined_df = pd.concat([grayscale_df, canny_df, log_df, random_df], keys=['Grayscale', 'Canny', 'DoG', 'Random'])\n",
    "# # Reset index\n",
    "# combined_df.reset_index(level=0, inplace=True)\n",
    "# combined_df.rename(columns={'level_0': 'Method'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8acccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_mean_sem(df):\n",
    "#     \"\"\"\n",
    "#     Calculate mean and standard error of the mean (sem) for each metric.\n",
    "#     \"\"\"\n",
    "#     metrics = ['mse', 'ssim', 'vgg_perceptual', 'lpips', 'fsim', 'dists', 'pieapp', 'mdsi', 'vsi', 'srsim', 'infer_time_s']\n",
    "#     results = []\n",
    "    \n",
    "#     for metric in metrics:\n",
    "#         mean = df[metric].mean()\n",
    "#         sem = df[metric].std() / np.sqrt(len(df))\n",
    "#         results.append({'metric': metric, 'mean': mean, 'sem': sem})\n",
    "    \n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# # Calculate mean and sem for each method\n",
    "# grayscale_metrics = calculate_mean_sem(grayscale_df)\n",
    "# canny_metrics = calculate_mean_sem(canny_df)\n",
    "# log_metrics = calculate_mean_sem(log_df)\n",
    "# random_metrics = calculate_mean_sem(random_df)\n",
    "# # Combine the metrics into a single DataFrame\n",
    "# combined_metrics = pd.concat([grayscale_metrics, canny_metrics, log_metrics, random_metrics], keys=['Grayscale', 'Canny', 'DoG', 'Random'])\n",
    "# # give a column name for the method\n",
    "# combined_metrics.reset_index(level=0, inplace=True)\n",
    "# combined_metrics.rename(columns={'level_0': 'method'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423f2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = combined_metrics['metric'].unique()\n",
    "# n = len(metrics)\n",
    "# cols = 2\n",
    "# rows = (n + 1)//cols\n",
    "\n",
    "# fig, axes = plt.subplots(rows, cols, figsize=(8*cols, 5*rows))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for ax, metric in zip(axes, metrics):\n",
    "#     dfm = combined_metrics[combined_metrics['metric'] == metric]\n",
    "#     ax.bar(dfm['method'], dfm['mean'], \n",
    "#            yerr=dfm['sem'], \n",
    "#            capsize=4)\n",
    "#     ax.set_title(metric)\n",
    "#     ax.set_ylabel('Score')\n",
    "#     ax.set_xticklabels(dfm['method'], rotation=45, ha='right')\n",
    "    \n",
    "# # hide any unused axes\n",
    "# for ax in axes[len(metrics):]:\n",
    "#     ax.axis('off')\n",
    "\n",
    "# # title is dataset\n",
    "# plt.suptitle(f'Metrics for {experiment_name} dataset', fontsize=16)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2248a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Imports and config ---\n",
    "# import os, math, json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from pathlib import Path\n",
    "# from scipy import stats\n",
    "\n",
    "# sns.set_context(\"talk\")\n",
    "\n",
    "# # Utility: safe load\n",
    "# def load_csv(fp):\n",
    "#     if os.path.exists(fp):\n",
    "#         return pd.read_csv(fp)\n",
    "#     print(f\"[warn] Missing: {fp}\")\n",
    "#     return None\n",
    "\n",
    "# # Where your results live\n",
    "# experiment_name = 'lapa'   # e.g. 'sun', 'lapa', 'coco'\n",
    "# base = Path(\"../results\")\n",
    "\n",
    "# # Optionally support multiple seeds per method: results/<method>_<dataset>/seed<N>/per_image_metrics.csv\n",
    "# def gather_method(method_tag, dataset, seeds=None):\n",
    "#     dfs = []\n",
    "#     if seeds is None:\n",
    "#         df = load_csv(str(base / f\"{method_tag}_{dataset}\" / \"per_image_metrics.csv\"))\n",
    "#         if df is not None:\n",
    "#             df[\"seed\"] = 0\n",
    "#             dfs.append(df)\n",
    "#     else:\n",
    "#         for s in seeds:\n",
    "#             df = load_csv(str(base / f\"{method_tag}_{dataset}\" / f\"seed{s}\" / \"per_image_metrics.csv\"))\n",
    "#             if df is not None:\n",
    "#                 df[\"seed\"] = s\n",
    "#                 dfs.append(df)\n",
    "#     if len(dfs) == 0:\n",
    "#         return None\n",
    "#     out = pd.concat(dfs, ignore_index=True)\n",
    "#     out[\"method\"] = method_tag.capitalize().replace(\"dog\",\"DoG\")\n",
    "#     return out\n",
    "\n",
    "# # Choose seeds if you have them. Otherwise set to None.\n",
    "# seeds = None  # e.g. [0,1,2,3,4]\n",
    "# methods = [\"grayscale\",\"canny\",\"DoG\",\"random\"]\n",
    "\n",
    "# dfs = []\n",
    "# for m in methods:\n",
    "#     d = gather_method(m, experiment_name, seeds)\n",
    "#     if d is not None:\n",
    "#         dfs.append(d)\n",
    "\n",
    "# assert len(dfs) >= 2, \"Need at least two methods loaded\"\n",
    "# df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# # If PSNR is not present, compute from MSE if you stored image dynamic range in your CSV\n",
    "# # Otherwise, skip gracefully\n",
    "# if \"psnr\" not in df_all.columns and \"mse\" in df_all.columns:\n",
    "#     # Assume images normalized to [0,1]\n",
    "#     df_all[\"psnr\"] = -10*np.log10(df_all[\"mse\"].clip(0))\n",
    "\n",
    "# # Keep a canonical metric list\n",
    "# metric_cols = [c for c in [\"mse\",\"ssim\",\"psnr\",\"fsim\",\"lpips\",\"dists\",\"pieapp\",\"mdsi\",\"vsi\",\"srsim\",\"vgg_perceptual\",\"infer_time_s\"] if c in df_all.columns]\n",
    "\n",
    "# # Sanity: per-image merge key\n",
    "# merge_keys = [c for c in [\"image_id\",\"filename\",\"name\"] if c in df_all.columns]\n",
    "# if len(merge_keys) == 0:\n",
    "#     # manufacture an image index per method group to enable pairing\n",
    "#     df_all[\"image_idx\"] = df_all.groupby(\"method\").cumcount()\n",
    "#     merge_keys = [\"image_idx\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a34ba3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mean_ci(x, alpha=0.05):\n",
    "#     x = np.asarray(x); x = x[~np.isnan(x)]\n",
    "#     n = len(x)\n",
    "#     if n == 0:\n",
    "#         return np.nan, np.nan, np.nan\n",
    "#     m = x.mean()\n",
    "#     se = x.std(ddof=1) / math.sqrt(n)\n",
    "#     t = stats.t.ppf(1 - alpha/2, df=max(n-1,1))\n",
    "#     return m, m - t*se, m + t*se\n",
    "\n",
    "# # For ranking we keep metric direction. Define whether higher is better.\n",
    "# higher_better = {\n",
    "#     \"mse\": False, \"ssim\": False, \"psnr\": True, \"fsim\": False,\n",
    "#     \"lpips\": False, \"dists\": False, \"pieapp\": False, \"mdsi\": False,\n",
    "#     \"vsi\": False, \"srsim\": False, \"vgg_perceptual\": False,  # VGG distance\n",
    "#     \"infer_time_s\": False\n",
    "# }\n",
    "\n",
    "# summary_rows = []\n",
    "# for met in metric_cols:\n",
    "#     tmp = df_all.groupby(\"method\")[met].apply(list).to_dict()\n",
    "#     for meth, arr in tmp.items():\n",
    "#         m, lo, hi = mean_ci(arr, alpha=0.05)\n",
    "#         summary_rows.append({\"metric\": met, \"method\": meth, \"mean\": m, \"ci_low\": lo, \"ci_high\": hi})\n",
    "\n",
    "# summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# # Rank within each metric\n",
    "# def rank_rows(sub):\n",
    "#     hb = higher_better[sub.name]\n",
    "#     sub = sub.sort_values(\"mean\", ascending=not hb).copy()\n",
    "#     sub[\"rank\"] = np.arange(1, len(sub)+1)\n",
    "#     return sub\n",
    "\n",
    "# ranked = summary_df.groupby(\"metric\", group_keys=False).apply(rank_rows)\n",
    "# ranked.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d254fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import combinations\n",
    "\n",
    "# def paired_stats(df, metric, methods, keys):\n",
    "#     # pivot per image for pairing\n",
    "#     sub = df[df[\"method\"].isin(methods)][keys + [\"method\", metric]].copy()\n",
    "#     piv = sub.pivot_table(index=keys, columns=\"method\", values=metric)\n",
    "#     piv = piv.dropna(axis=0)  # drop images missing any method\n",
    "\n",
    "#     pairs = []\n",
    "#     for a,b in combinations(methods, 2):\n",
    "#         xa = piv[a].values\n",
    "#         xb = piv[b].values\n",
    "#         t, p = stats.ttest_rel(xa, xb, nan_policy=\"omit\")\n",
    "#         # Cohen's dz for paired design\n",
    "#         d = (xa - xb).mean() / (xa - xb).std(ddof=1)\n",
    "#         pairs.append({\"metric\": metric, \"A\": a, \"B\": b, \"t\": t, \"p_raw\": p, \"dz\": d, \"n\": len(xa)})\n",
    "#     out = pd.DataFrame(pairs)\n",
    "#     # Holm correction\n",
    "#     out = out.sort_values(\"p_raw\").reset_index(drop=True)\n",
    "#     m = len(out)\n",
    "#     out[\"p_holm\"] = [min(1.0, out.loc[i,\"p_raw\"]*(m-i)) for i in range(m)]\n",
    "#     return out\n",
    "\n",
    "# tests = []\n",
    "# for met in metric_cols:\n",
    "#     tests.append(paired_stats(df_all, met, sorted(df_all[\"method\"].unique()), merge_keys))\n",
    "# tests_df = pd.concat(tests, ignore_index=True)\n",
    "# tests_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85b13c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote tables/recon_lapa.tex\n",
      "Wrote tables/recon_sun.tex\n",
      "Wrote tables/recon_coco.tex\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config\n",
    "# ---------------------------------------------------------------------\n",
    "DATASETS = [\"lapa\", \"sun\", \"coco\"]          # change as needed\n",
    "METHODS  = [\"DoG\", \"canny\", \"random\"]  # folder prefixes\n",
    "\n",
    "# Metric groups for the table header\n",
    "INTENSITY_METRICS  = [\"mse\", \"ssim\", \"psnr\"]\n",
    "PERCEPTUAL_METRICS = [\"lpips\", \"dists\", \"mdsi\", \"vsi\"]  # add \"vgg_perceptual\" if you log it\n",
    "\n",
    "# Direction of improvement\n",
    "HIGHER_BETTER = {\n",
    "    \"mse\": False, \"ssim\": False, \"psnr\": True, \"fsim\": False,\n",
    "    \"lpips\": False, \"dists\": False, \"pieapp\": False, \"mdsi\": False,\n",
    "    \"vsi\": False, \"srsim\": False, \"vgg_perceptual\": False\n",
    "}\n",
    "\n",
    "# Pretty names for LaTeX header\n",
    "PRETTY = {\n",
    "    \"mse\":\"MSE\", \"ssim\":\"SSIM\", \"psnr\":\"PSNR\",\n",
    "    \"lpips\":\"LPIPS\", \"dists\":\"DISTS\", \"pieapp\":\"PIEAPP\", \"mdsi\":\"MDSI\", \"vsi\":\"VSI\", \"srsim\":\"SRSIM\",\n",
    "    \"vgg_perceptual\":\"VGG perc.\"\n",
    "}\n",
    "PRETTY_METHOD = {\n",
    "    \"grayscale\":\"Grayscale\", \"canny\":\"Canny\", \"DoG\":\"SCAPE\", \"random\":\"Random\"\n",
    "}\n",
    "\n",
    "RESULTS_DIR = Path(\"../results\")   # adjust if needed\n",
    "OUT_DIR = Path(\"./tables\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def load_per_image_metrics(method_tag, dataset):\n",
    "    \"\"\"Load ../results/<method>_<dataset>/per_image_metrics.csv\"\"\"\n",
    "    fp = RESULTS_DIR / f\"{method_tag}_{dataset}\" / \"per_image_metrics.csv\"\n",
    "    if not fp.exists():\n",
    "        print(f\"[warn] missing {fp}\")\n",
    "        return None\n",
    "    df = pd.read_csv(fp)\n",
    "    df[\"method\"] = PRETTY_METHOD.get(method_tag, method_tag)\n",
    "    return df\n",
    "\n",
    "def ci95(values):\n",
    "    x = np.asarray(values); x = x[~np.isnan(x)]\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan\n",
    "    m = x.mean()\n",
    "    se = x.std(ddof=1) / math.sqrt(n)\n",
    "    t = stats.t.ppf(0.975, df=max(n-1, 1))\n",
    "    half = t * se\n",
    "    return m, half\n",
    "\n",
    "def format_entry(mean, half):\n",
    "    if np.isnan(mean):\n",
    "        return \"--\"\n",
    "    return f\"{mean:.3f}\"\n",
    "\n",
    "def bold_best(entries, higher_better=True):\n",
    "    \"\"\"entries is a list of floats (means). returns set of indices that are best\"\"\"\n",
    "    arr = np.array(entries, dtype=float)\n",
    "    if np.all(np.isnan(arr)):\n",
    "        return set()\n",
    "    if higher_better:\n",
    "        best_val = np.nanmax(arr)\n",
    "    else:\n",
    "        best_val = np.nanmin(arr)\n",
    "    # allow tiny ties\n",
    "    eps = 1e-12\n",
    "    return set(np.where(np.isclose(arr, best_val, atol=eps))[0].tolist())\n",
    "\n",
    "def available_metrics(df, wanted):\n",
    "    return [m for m in wanted if m in df.columns]\n",
    "\n",
    "def build_table_for_dataset(dataset):\n",
    "    # load all methods\n",
    "    dfs = []\n",
    "    for m in METHODS:\n",
    "        d = load_per_image_metrics(m, dataset)\n",
    "        if d is not None:\n",
    "            dfs.append(d)\n",
    "    if not dfs:\n",
    "        print(f\"[warn] no data for {dataset}\")\n",
    "        return None\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # derive PSNR from MSE if missing and images are in [0,1]\n",
    "    if \"psnr\" not in df.columns and \"mse\" in df.columns:\n",
    "        df[\"psnr\"] = -10 * np.log10(df[\"mse\"].clip(1e-12))\n",
    "\n",
    "    imetrics  = available_metrics(df, INTENSITY_METRICS)\n",
    "    pmetrics  = available_metrics(df, PERCEPTUAL_METRICS)\n",
    "    all_metrics = imetrics + pmetrics\n",
    "\n",
    "    # compute mean and CI per method per metric\n",
    "    summary = {}\n",
    "    for method_name, dsub in df.groupby(\"method\"):\n",
    "        row = {}\n",
    "        for met in all_metrics:\n",
    "            m, h = ci95(dsub[met].values)\n",
    "            row[met] = (m, h)\n",
    "        summary[method_name] = row\n",
    "\n",
    "    # decide best per metric for bolding\n",
    "    best_indices = {}\n",
    "    method_list = [PRETTY_METHOD.get(m, m) for m in METHODS if PRETTY_METHOD.get(m, m) in summary]\n",
    "    for met in all_metrics:\n",
    "        means = [summary[m][met][0] for m in method_list]\n",
    "        best_indices[met] = bold_best(means, higher_better=HIGHER_BETTER.get(met, False))\n",
    "\n",
    "    # build LaTeX\n",
    "    def header_block(title, metrics):\n",
    "        cols = \" \".join([PRETTY[m] for m in metrics])\n",
    "        return f\"  & \\\\multicolumn{{{len(metrics)}}}{{c}}{{{title}}} \\\\\\\\\"\n",
    "\n",
    "    def cmidrule(start_col, width):\n",
    "        return f\"\\\\cmidrule(lr){{{start_col}-{start_col+width-1}}}\"\n",
    "\n",
    "    n_int = len(imetrics)\n",
    "    n_per = len(pmetrics)\n",
    "    # column layout: 1 (Processing model) + n_int + n_per\n",
    "    colspec = \"l\" + \"c\"*n_int + \" \" + \"c\"*n_per\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"\\\\begin{table}[t]\")\n",
    "    lines.append(\"\\\\centering\")\n",
    "    lines.append(\"\\\\small\")\n",
    "    lines.append(f\"\\\\caption{{Reconstruction metrics on the {dataset.upper()} dataset. Lower is better for MSE, LPIPS, DISTS, PIEAPP, MDSI, VGG perceptual. Higher is better for SSIM, PSNR, FSIM, VSI, SRSIM. Best per column in bold.}}\")\n",
    "    lines.append(f\"\\\\label{{tab:recon_{dataset}}}\")\n",
    "    lines.append(\"\\\\begin{tabular}{\" + colspec + \"}\")\n",
    "    lines.append(\"\\\\toprule\")\n",
    "    # first header row with groups\n",
    "    left = \"Processing model \"\n",
    "    right_groups = []\n",
    "    if n_int > 0:\n",
    "        right_groups.append(f\"\\\\multicolumn{{{n_int}}}{{c}}{{Intensity reconstruction}}\")\n",
    "    if n_per > 0:\n",
    "        right_groups.append(f\"\\\\multicolumn{{{n_per}}}{{c}}{{Perceptual reconstruction}}\")\n",
    "    lines.append(left + \" & \" + \" & \".join(right_groups) + \" \\\\\\\\\")\n",
    "    # cmidrules\n",
    "    start = 2\n",
    "    rules = []\n",
    "    if n_int > 0:\n",
    "        rules.append(cmidrule(start, n_int))\n",
    "        start += n_int\n",
    "    if n_per > 0:\n",
    "        rules.append(cmidrule(start, n_per))\n",
    "    lines.append(\" \".join(rules))\n",
    "    # second header row with metric names\n",
    "    header_metrics = [PRETTY[m] for m in imetrics + pmetrics]\n",
    "    lines.append(\"  & \" + \" & \".join(header_metrics) + \" \\\\\\\\\")\n",
    "    lines.append(\"\\\\midrule\")\n",
    "\n",
    "    # rows\n",
    "    for i, method_name in enumerate(method_list):\n",
    "        row = [method_name]\n",
    "        for j, met in enumerate(all_metrics):\n",
    "            mean, half = summary[method_name][met]\n",
    "            cell = format_entry(mean, half)\n",
    "            if i in best_indices[met]:\n",
    "                cell = f\"\\\\textbf{{{cell}}}\"\n",
    "            row.append(cell)\n",
    "        lines.append(\" & \".join(row) + \" \\\\\\\\\")\n",
    "    lines.append(\"\\\\bottomrule\")\n",
    "    lines.append(\"\\\\end{tabular}\")\n",
    "    lines.append(\"\\\\end{table}\")\n",
    "\n",
    "    latex = \"\\n\".join(lines)\n",
    "    out_path = OUT_DIR / f\"recon_{dataset}.tex\"\n",
    "    out_path.write_text(latex, encoding=\"utf-8\")\n",
    "    print(f\"Wrote {out_path}\")\n",
    "    return latex\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Run for all datasets\n",
    "# ---------------------------------------------------------------------\n",
    "latex_by_ds = {ds: build_table_for_dataset(ds) for ds in DATASETS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45985527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
