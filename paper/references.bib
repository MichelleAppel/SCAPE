@article {vanderGrinten2024,
article_type = {journal},
title = {Towards biologically plausible phosphene simulation for the differentiable optimization of visual cortical prostheses},
author = {van der Grinten, Maureen and de Ruyter van Steveninck, Jaap and Lozano, Antonio and Pijnacker, Laura and Rueckauer, Bodo and Roelfsema, Pieter and van Gerven, Marcel and van Wezel, Richard and Güçlü, Umut and Güçlütürk, Yağmur},
editor = {Baker, Chris I and Barry, Michael P},
volume = 13,
year = 2024,
month = {feb},
pub_date = {2024-02-22},
pages = {e85812},
citation = {eLife 2024;13:e85812},
doi = {10.7554/eLife.85812},
url = {https://doi.org/10.7554/eLife.85812},
abstract = {Blindness affects millions of people around the world. A promising solution to restoring a form of vision for some individuals are cortical visual prostheses, which bypass part of the impaired visual pathway by converting camera input to electrical stimulation of the visual system. The artificially induced visual percept (a pattern of localized light flashes, or ‘phosphenes’) has limited resolution, and a great portion of the field’s research is devoted to optimizing the efficacy, efficiency, and practical usefulness of the encoding of visual information. A commonly exploited method is non-invasive functional evaluation in sighted subjects or with computational models by using simulated prosthetic vision (SPV) pipelines. An important challenge in this approach is to balance enhanced perceptual realism, biologically plausibility, and real-time performance in the simulation of cortical prosthetic vision. We present a biologically plausible, PyTorch-based phosphene simulator that can run in real-time and uses differentiable operations to allow for gradient-based computational optimization of phosphene encoding models. The simulator integrates a wide range of clinical results with neurophysiological evidence in humans and non-human primates. The pipeline includes a model of the retinotopic organization and cortical magnification of the visual cortex. Moreover, the quantitative effects of stimulation parameters and temporal dynamics on phosphene characteristics are incorporated. Our results demonstrate the simulator’s suitability for both computational applications such as end-to-end deep learning-based prosthetic vision optimization as well as behavioral experiments. The modular and open-source software provides a flexible simulation framework for computational, clinical, and behavioral neuroscientists working on visual neuroprosthetics.},
keywords = {simulated prosthetic vision, cortical stimulation, bionic vision, blindness, deep learning, neurotechnology, neural implants},
journal = {eLife},
issn = {2050-084X},
publisher = {eLife Sciences Publications, Ltd},
}

@article{Polimeni2006,
title = {Multi-area visuotopic map complexes in macaque striate and extra-striate cortex},
journal = {Vision Research},
volume = {46},
number = {20},
pages = {3336-3359},
year = {2006},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2006.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0042698906001428},
author = {J.R. Polimeni and M. Balasubramanian and E.L. Schwartz},
keywords = {Visual cortex, Retinotopic, Quasiconformal mapping, Topographic modeling, Topographic map complex},
abstract = {We propose that a simple, closed-form mathematical expression—the Wedge–Dipole mapping—provides a concise approximation to the full-field, two-dimensional topographic structure of macaque V1, V2, and V3. A single map function, which we term a map complex, acts as a simultaneous descriptor of all three areas. Quantitative estimation of the Wedge–Dipole parameters is provided via 2DG data of central-field V1 topography and a publicly available data set of full-field macaque V1 and V2 topography. Good quantitative agreement is obtained between the data and the model presented here. The increasing importance of fMRI-based brain imaging motivates the development of more sophisticated two-dimensional models of cortical visuotopy, in contrast to the one-dimensional approximations that have been in common use. One reason is that topography has traditionally supplied an important aspect of “ground truth,” or validation, for brain imaging, suggesting that further development of high-resolution fMRI will be facilitated by this data analysis. In addition, several important insights into the nature of cortical topography follow from this work. The presence of anisotropy in cortical magnification factor is shown to follow mathematically from the shared boundary conditions at the V1–V2 and V2–V3 borders, and therefore may not causally follow from the existence of columnar systems in these areas, as is widely assumed. An application of the Wedge–Dipole model to localizing aspects of visual processing to specific cortical areas—extending previous work in correlating V1 cortical magnification factor to retinal anatomy or visual psychophysics data—is briefly discussed.}
}

@ARTICLE{Nyquist1928,
  author={Nyquist, H.},
  journal={Transactions of the American Institute of Electrical Engineers}, 
  title={Certain Topics in Telegraph Transmission Theory}, 
  year={1928},
  volume={47},
  number={2},
  pages={617-644},
  keywords={Telegraphy;Steady-state;Frequency conversion;Circuits;Costs;Distortion;Shape;Interference;Equalizers;Telephony},
  doi={10.1109/T-AIEE.1928.5055024}}

@article {Young1987,
      author = "Richard A. Young",
      title = "The Gaussian derivative model for spatial vision: I. Retinal mechanisms",
      journal = "Spatial Vision",
      year = "1987",
      publisher = "Brill",
      address = "Leiden, The Netherlands",
      volume = "2",
      number = "4",
      doi = "10.1163/156856887X00222",
      pages=      "273 - 293",
      url = "https://brill.com/view/journals/sv/2/4/article-p273_3.xml"
}

@article {deRuytervanSteveninck2020,
	author = {de Ruyter van Steveninck, Jaap and G{\"u}{\c c}l{\"u}, Umut and van Wezel, Richard and van Gerven, Marcel},
	title = {End-to-end optimization of prosthetic vision},
	elocation-id = {2020.12.19.423601},
	year = {2020},
	doi = {10.1101/2020.12.19.423601},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Neural prosthetics may provide a promising solution to restore visual perception in some forms of blindness. The restored prosthetic percept is rudimentary compared to normal vision and can be optimized with a variety of image preprocessing techniques to maximize relevant information transfer. Extracting the most useful features from a visual scene is a non-trivial task and optimal preprocessing choices strongly depend on the context. Despite rapid advancements in deep learning, research currently faces a difficult challenge in finding a general and automated preprocessing strategy that can be tailored to specific tasks or user requirements. In this paper we present a novel deep learning approach that explicitly addresses this issue by optimizing the entire process of phosphene generation in an end-to-end fashion. The proposed model is based on a deep auto-encoder architecture and includes a highly adjustable simulation module of prosthetic vision. In computational validation experiments we show that such an approach is able to automatically find a task-specific stimulation protocol. The presented approach is highly modular and could be extended to dynamically optimize prosthetic vision for everyday tasks and requirements of the end-user.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2020/12/21/2020.12.19.423601},
	eprint = {https://www.biorxiv.org/content/early/2020/12/21/2020.12.19.423601.full.pdf},
	journal = {bioRxiv}
}


@misc{Ronneberger2015,
      title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
      author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
      year={2015},
      eprint={1505.04597},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1505.04597}, 
}

@misc{Hu2018,
      title={Squeeze-and-Excitation Networks}, 
      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},
      year={2019},
      eprint={1709.01507},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1709.01507}, 
}

@misc{Yu2016,
      title={Multi-Scale Context Aggregation by Dilated Convolutions}, 
      author={Fisher Yu and Vladlen Koltun},
      year={2016},
      eprint={1511.07122},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1511.07122}, 
}

@misc{Oktay2018,
      title={Attention U-Net: Learning Where to Look for the Pancreas}, 
      author={Ozan Oktay and Jo Schlemper and Loic Le Folgoc and Matthew Lee and Mattias Heinrich and Kazunari Misawa and Kensaku Mori and Steven McDonagh and Nils Y Hammerla and Bernhard Kainz and Ben Glocker and Daniel Rueckert},
      year={2018},
      eprint={1804.03999},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1804.03999}, 
}

@article{ZiaeiNafchi2016,
   title={Mean Deviation Similarity Index: Efficient and Reliable Full-Reference Image Quality Evaluator},
   volume={4},
   ISSN={2169-3536},
   url={http://dx.doi.org/10.1109/ACCESS.2016.2604042},
   DOI={10.1109/access.2016.2604042},
   journal={IEEE Access},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Ziaei Nafchi, Hossein and Shahkolaei, Atena and Hedjam, Rachid and Cheriet, Mohamed},
   year={2016},
   pages={5579–5590} }


@ARTICLE{Wang2004,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
  doi={10.1109/TIP.2003.819861}}

@ARTICLE{Zhang2014,
  author={Zhang, Lin and Shen, Ying and Li, Hongyu},
  journal={IEEE Transactions on Image Processing}, 
  title={VSI: A Visual Saliency-Induced Index for Perceptual Image Quality Assessment}, 
  year={2014},
  volume={23},
  number={10},
  pages={4270-4281},
  keywords={Visualization;Computational modeling;Indexes;Image quality;Measurement;Image color analysis;Feature extraction;Perceptual image quality assessment;visual saliency},
  doi={10.1109/TIP.2014.2346028}}

@misc{Prashnani2018,
    title={PieAPP: Perceptual Image-Error Assessment through Pairwise Preference}, 
    author={Ekta Prashnani and Hong Cai and Yasamin Mostofi and Pradeep Sen},
    year={2018},
    eprint={1806.02067},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/1806.02067}, 
}

@article{Wang2018, 
author = {Wang, Xiaosha and Xu, Yangwen and Wang, Yuwei and Zeng, Yi and Zhang, Jiacai and Ling, Zhen-Hua and Bi, Yanchao},
year = {2018},
month = {02},
pages = {},
title = {Representational similarity analysis reveals task-dependent semantic influence of the visual word form area},
volume = {8},
journal = {Scientific Reports},
doi = {10.1038/s41598-018-21062-0}
}

@article{Kriegeskorte2008,
author = {Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter},
year = {2008},
month = {02},
pages = {4},
title = {Representational Similarity Analysis – Connecting the Branches of Systems Neuroscience},
volume = {2},
journal = {Frontiers in systems neuroscience},
doi = {10.3389/neuro.06.004.2008}
}