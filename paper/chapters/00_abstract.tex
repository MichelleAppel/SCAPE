Visual cortical implants aim to restore sight by electrically stimulating neurons with electrode arrays. Conventional encoding methods typically apply uniform filters across the entire visual field, ignoring the uneven sampling imposed by implant layouts. This mismatch can oversmooth detail in dense regions or introduce clutter where coverage is sparse. We present SCAPE (Shift-variant Cortical-implant Adaptive Phosphene Encoding), a framework that adapts image processing to local electrode density. Electrode coordinates are projected into visual-field space to estimate sampling density via cortical magnification models or kernel density estimation. Nyquist principles then convert density into a spatial scale map, guiding shift-variant filtering whose kernel width matches local resolution limits. We demonstrate an efficient separable Difference-of-Gaussians implementation, though SCAPE generalizes to other kernels. Integrated with a reconstruction decoder, SCAPE consistently preserves structural detail and improves reconstruction quality across diverse datasets and implant schemes. By explicitly linking electrode layout to adaptive spatial filtering, SCAPE provides a lightweight and principled foundation for enhancing prosthetic vision and accelerating translation toward clinical use.