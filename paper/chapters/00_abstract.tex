Visual cortical implants aim to restore a form of sight by electrically stimulating neurons with electrode arrays. Each of the implant's contact point potentially generates a visual percept or ‘‘phosphene’’, in a specific location of the visual field, according to the visual area's retinotopy.

To convey useful visual information to the implant's users, conventional computer vision image encoding methods typically apply uniform filters across the entire visual field, ignoring the uneven phosphene map distribution imposed by implant layouts. 
This mismatch can oversmooth detail in dense regions or introduce clutter where coverage is sparse.

To overcome this limitation, we present SCAPE (Shift-variant Cortical-implant Adaptive Phosphene Encoding), a framework that adapts image processing to local electrode density. 
Electrode cortical locations are projected into visual-field space to estimate sampling density via cortical magnification models or kernel density estimation. Nyquist principles then convert phosphene density into a spatial scale map, guiding shift-variant filtering whose kernel width matches local resolution limits.

We demonstrate an efficient separable Difference-of-Gaussians implementation, though SCAPE generalizes to other kernels. Integrated with a reconstruction decoder, SCAPE consistently preserves structural detail of the generated phosphene images and improves reconstruction quality across diverse datasets and implant schemes. By explicitly linking electrode layout to adaptive spatial filtering, SCAPE provides a computationally lightweight and principled foundation for enhancing prosthetic vision and accelerating translation toward clinical use.