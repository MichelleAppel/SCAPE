\section{Related Work}
Efficiently encoding complex scenes for visual prostheses has progressed from simple contrast and edge detection to adaptive heuristics and end-to-end learned encoders. However, existing methods process the visual field uniformly and do not account for patient-specific electrode layouts or cortical magnification. This section reviews the evolution of image processing techniques for visual prostheses, highlighting the limitations of current approaches and the need for adaptive methods like SCAPE.

\subsection{Image Processing for Visual Prostheses}
Effective image processing is critical for maximizing the limited information conveyed by cortical visual prostheses. Traditional and recent methods have focused on extracting salient features or learning stimulus encoders, but none explicitly account for the patientâ€™s electrode layout or variations in cortical magnification. Ignoring these factors can lead to oversmoothing where electrodes are dense and visual clutter where electrodes are sparse. SCAPE addresses this gap by adapting filtering to local sampling density.

\subsubsection{Early Heuristic Methods}
Under severe electrode count and bandwidth constraints, early approaches applied contrast and brightness enhancement, grayscale histogram equalization, and gradient-based edge detection operators such as Sobel and Canny to extract salient contours in low-resolution phosphene maps \cite{Liu2005,Oh2024}. These methods often binarized or morphologically filtered the output to suppress noise and highlight obstacles or object boundaries. However, they treated the visual field uniformly and did not consider implant-specific sampling schemes or the cortical magnification factor.

\subsubsection{More Recent Learned Encodings}
More recent strategies frame stimulus encoding as an optimization or learning problem. Relic et al.\ trained a convolutional neural network encoder in an end-to-end fashion with a differentiable phosphene simulator to predict electrode activation patterns for intelligible phosphenes on MNIST images \cite{Relic2022}. Granley et al.\ developed hybrid neural autoencoders that invert neuroscientific forward models to generate patient-specific stimulation strategies, demonstrating improved fidelity over conventional encodings \cite{Granley2022}. De Ruyter van Steveninck .\ proposed an end-to-end learned encoder that jointly optimizes spatial filtering and electrode selection, but still does not explicitly modulate processing based on local sampling density or cortical magnification \cite{deRuytervanSteveninck2020}.

\subsection{Simulated Prosthetic Vision Pipelines}
Simulated prosthetic vision pipelines convert visual inputs into phosphene maps by modeling the response to electrode stimulation. Early implementations often used static Gaussian phosphenes without explicit retinotopic mapping or cortical magnification, due to limited computational frameworks at the time. Immersive VR-SPV systems enabled user studies in virtual environments but relied on non-differentiable heuristics and uniform phosphene shapes, which constrained encoder development \cite{Kasowski2022}. More recently, van der Grinten et al.\ released a fully differentiable PyTorch simulator that incorporates retinotopic projection, cortical magnification, current spread, temporal dynamics, and support for arbitrary electrode layouts, facilitating real-time rendering and gradient-based optimization of encoding algorithms \cite{vanderGrinten2024}.

\subsection{Adaptive and Spatially-Varying Encoding Approaches}
Several recent methods adapt encoding globally or per patient but still treat spatial filtering uniformly. Granley et al.\ introduced a human-in-the-loop Bayesian optimization framework that personalizes deep encoder parameters to individual patients, yielding improved perceived quality with minimal feedback \cite{Granley2023}. Hybrid neural autoencoders have been used to invert biophysical forward models in an end-to-end fashion and generate patient-specific stimulation patterns \cite{Granley2022, deRuytervanSteveninck2020}. Despite these advances, no existing approach explicitly modulates the spatial scale of processing according to local electrode density or Nyquist limits. SCAPE fills this gap by continuously adapting its filtering kernel to the sampling density of each implant.
